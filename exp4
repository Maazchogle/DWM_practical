Aim: Implementation of Naive Bayes Classifier.



Theory:

Naïve Bayes algorithm is a supervised learning algorithm, which is based on Bayes
theorem and used for solving classification problems. The Naïve Bayes algorithm is
comprised of two words Naïve and Bayes. It is mainly used in text classification that
includes a high-dimensional training dataset. Naïve Bayes Classifier is one of the simple
and most effective Classification algorithms which helps in building the fast machine
learning models that can make quick predictions. It is a probabilistic classifier, which
means it predicts on the basis of the probability of an object. It is called Naïve because
it assumes that the occurrence of a certain feature is independent of the occurrence of
other features. Such as if the fruit is identified on the bases of color, shape, and taste,
then red, spherical, and sweet fruit is recognized as an apple. Hence each feature
individually contributes to identify that it is an apple without depending on each other.
It is called Bayes because it depends on the principle of Bayes’ Theorem. Bayes'
theorem is also known as Bayes' Rule or Bayes' law, which is used to determine the
probability of a hypothesis with prior knowledge. It depends on the conditional
probability. It is not a single algorithm but a family of algorithms where all of them
share a common principle, i.e. every pair of features being classified is independent of
each other.

Program:
import numpy as np
from sklearn.naive_bayes import GaussianNB
# Define the Tennis dataset
# Format: Outlook, Temperature, Humidity, Windy, PlayTennis
# 0: Sunny, 1: Overcast, 2: Rainy
# 0: Cool, 1: Mild, 2: Hot
# 0: Normal, 1: High
# 0: False, 1: True
# 0: No, 1: Yes
tennis_data = np.array(
[
[0, 0, 0, 0, 1],
[0, 0, 0, 1, 1],
[1, 0, 0, 0, 1],
[2, 1, 0, 0, 1],
[2, 2, 1, 0, 1],
[2, 2, 1, 1, 0],
[1, 2, 1, 1, 1],
[0, 1, 0, 0, 0],
[0, 2, 1, 0, 1],
[2, 1, 1, 0, 1],
[0, 1, 1, 1, 1],
[1, 1, 0, 1, 1],
[1, 0, 1, 0, 1],
[2, 1, 0, 1, 0],
]
)
# Features: Outlook, Temperature, Humidity, Windy
X = tennis_data[:, :-1]
# Target: PlayTennis
y = tennis_data[:, -1]
# Train the Naive Bayes classifier
clf = GaussianNB()
clf.fit(X, y)
# Take user input for weather conditions
print("\nEnter weather conditions:")
outlook = int(input("\nOutlook (0: Sunny, 1: Overcast, 2: Rainy): "))

temperature = int(input("\nTemperature (0: Cool, 1: Mild, 2: Hot): "))
humidity = int(input("\nHumidity (0: Normal, 1: High): "))
windy = int(input("\nWindy (0: False, 1: True): "))
# Predict whether to play tennis
user_input = np.array([[outlook, temperature, humidity, windy]])
predicted_play = clf.predict(user_input)
# Display the prediction
play_decision = "Yes" if predicted_play == 1 else "No"
print("\nPredicted decision to play cricket:", play_decision)

Output:

Conclusion:
In conclusion, the experiment involving the Naive Bayes classifier demonstrated its
effectiveness in classifying data based on probabilistic assumptions and independence
assumptions between features. The classifier showed promising performance in terms
of speed and efficiency, making it a viable choice for text classification, spam
filtering, and other similar applications.
